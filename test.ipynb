{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33860391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Opening camera...\n",
      "Camera opened successfully.\n",
      "Cleaning up and closing...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# --- NEW: IMPORT THE CORRECT PREPROCESSOR ---\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# --- 1. LOAD YOUR TRAINED MODEL ---\n",
    "try:\n",
    "    model = load_model('model1.keras') # Make sure this is the new MobileNetV2 model\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Make sure 'model1.keras' is in the correct path.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. DEFINE YOUR CLASS NAMES ---\n",
    "class_names = [\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "    'del', 'nothing', 'space'\n",
    "]\n",
    "\n",
    "# --- 3. SET UP WEBCAM (WITH WSL FIXES) ---\n",
    "print(\"Opening camera...\")\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_V4L2) \n",
    "cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "print(\"Camera opened successfully.\")\n",
    "\n",
    "# --- 4. DEFINE THE REGION OF INTEREST (ROI) ---\n",
    "ROI_TOP = 50\n",
    "ROI_BOTTOM = 350\n",
    "ROI_RIGHT = 350\n",
    "ROI_LEFT = 650\n",
    "IMG_SIZE = (200, 200) # Match your training\n",
    "\n",
    "# --- 5. PERFORMANCE OPTIMIZATION VARS ---\n",
    "frame_counter = 0 \n",
    "current_label = \"\" \n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Can't receive frame. Exiting.\")\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_counter += 1\n",
    "    cv2.rectangle(frame, (ROI_LEFT, ROI_TOP), (ROI_RIGHT, ROI_BOTTOM), (0, 255, 0), 2)\n",
    "    \n",
    "    # --- 6. PRE-PROCESS AND PREDICT (ONLY EVERY 5 FRAMES) ---\n",
    "    if frame_counter % 5 == 0: \n",
    "        try:\n",
    "            roi = frame[ROI_TOP:ROI_BOTTOM, ROI_RIGHT:ROI_LEFT]\n",
    "            img = cv2.resize(roi, IMG_SIZE)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # This is correct\n",
    "            \n",
    "            # --- !!! THIS IS THE FIX !!! ---\n",
    "            \n",
    "            # 1. Expand dimensions to create a \"batch\" of 1\n",
    "            # We must do this *before* preprocessing\n",
    "            img_batch = np.expand_dims(img, axis=0)\n",
    "            \n",
    "            # 2. Apply the *exact same* preprocessing as in training\n",
    "            # This function converts [0, 255] -> [-1, 1]\n",
    "            img_preprocessed = preprocess_input(img_batch)\n",
    "            \n",
    "            # --- 7. MAKE A PREDICTION ---\n",
    "            # Feed the correctly preprocessed batch to the model\n",
    "            prediction = model.predict(img_preprocessed, verbose=0) \n",
    "\n",
    "            # --- (Rest of your prediction logic is correct) ---\n",
    "            pred_index = np.argmax(prediction[0])\n",
    "            pred_label = class_names[pred_index]\n",
    "            confidence = prediction[0][pred_index] * 100\n",
    "            \n",
    "            current_label = f\"{pred_label} ({confidence:.2f}%)\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # --- 8. DISPLAY THE PREDICTION ON *EVERY* FRAME ---\n",
    "    cv2.putText(\n",
    "        frame, \n",
    "        current_label,\n",
    "        (ROI_RIGHT - 20, ROI_TOP - 10), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "        0.7, \n",
    "        (0, 255, 0), \n",
    "        2\n",
    "    )\n",
    "        \n",
    "    cv2.imshow('ASL Prediction - Press \"q\" to quit', frame)\n",
    "\n",
    "    # --- 9. SET UP EXIT KEY ---\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# --- 10. CLEAN UP ---\n",
    "print(\"Cleaning up and closing...\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b765fca3",
   "metadata": {},
   "source": [
    "## Photo testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0027028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "The model predicts: A (100.00%)\n",
      "Press any key to close the image.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# --- 1. SET THE PATH TO YOUR IMAGE ---\n",
    "# !!! IMPORTANT: Change this to the path of the image you want to test\n",
    "IMAGE_PATH = 'asl_alphabet_test/A_test.jpg' \n",
    "# For example: '/home/sarvesh/projects/HPR/asl_alphabet_test/A/A_test.jpg'\n",
    "\n",
    "# --- 2. LOAD YOUR TRAINED MODEL ---\n",
    "try:\n",
    "    model = load_model('best_asl_model.keras')\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. DEFINE YOUR CLASS NAMES ---\n",
    "class_names = [\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "    'del', 'nothing', 'space'\n",
    "]\n",
    "\n",
    "# --- 4. DEFINE IMAGE SIZE ---\n",
    "IMG_SIZE = (200, 200) # Must match your model's input size\n",
    "\n",
    "# --- 5. LOAD AND PRE-PROCESS THE IMAGE ---\n",
    "# Load the image from disk\n",
    "image = cv2.imread(IMAGE_PATH)\n",
    "\n",
    "if image is None:\n",
    "    print(f\"Error: Could not load image from {IMAGE_PATH}\")\n",
    "    exit()\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "try:\n",
    "    # --- This part is the same as your webcam script's \"try\" block ---\n",
    "    \n",
    "    # Resize the image to match the model's expected input size\n",
    "    img_resized = cv2.resize(image, IMG_SIZE)\n",
    "    \n",
    "    # Rescale the image (0-255 -> 0-1)\n",
    "    img_rescaled = img_resized / 255.0\n",
    "    \n",
    "    # Expand dimensions to create a \"batch\" of 1\n",
    "    # Model expects shape (1, 200, 200, 3)\n",
    "    img_batch = np.expand_dims(img_rescaled, axis=0)\n",
    "\n",
    "    # --- 6. MAKE A PREDICTION ---\n",
    "    prediction = model.predict(img_batch, verbose=0)\n",
    "    \n",
    "    # Get the predicted class index\n",
    "    pred_index = np.argmax(prediction[0])\n",
    "    \n",
    "    # Get the corresponding class name\n",
    "    pred_label = class_names[pred_index]\n",
    "    \n",
    "    # Get the confidence score\n",
    "    confidence = prediction[0][pred_index] * 100\n",
    "    \n",
    "    text = f\"{pred_label} ({confidence:.2f}%)\"\n",
    "    print(f\"The model predicts: {text}\")\n",
    "\n",
    "    # --- 7. DISPLAY THE PREDICTION ON THE IMAGE ---\n",
    "    \n",
    "    # Make the original image a bit bigger for display\n",
    "    display_image = cv2.resize(image, (500, 500), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Put the prediction text on the display image\n",
    "    cv2.putText(\n",
    "        display_image, \n",
    "        text, \n",
    "        (20, 40), # Position text at the top-left\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "        1.0, \n",
    "        (0, 255, 0), # Green text\n",
    "        2\n",
    "    )\n",
    "    \n",
    "    # Show the image in a new window\n",
    "    cv2.imshow(f\"Prediction: {text}\", display_image)\n",
    "    \n",
    "    print(\"Press any key to close the image.\")\n",
    "    cv2.waitKey(0) # Wait indefinitely for a key press\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06cbbade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 11:25:37.129835: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-11 11:25:37.212056: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-11 11:25:38.477505: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762860339.194712  760053 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5561 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASL model loaded successfully!\n",
      "Opening camera...\n",
      "Camera opened successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"\"\n",
      "2025-11-11 11:25:51.982726: I external/local_xla/xla/service/service.cc:163] XLA service 0x704a580045e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-11 11:25:51.982761: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-11-11 11:25:51.992158: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-11 11:25:52.056408: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91500\n",
      "I0000 00:00:1762860353.373500  760189 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up and closing...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# --- 1. LOAD YOUR ASL MODEL ---\n",
    "try:\n",
    "    asl_model = load_model('model1.keras')\n",
    "    print(\"ASL model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ASL model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. DEFINE CONSTANTS ---\n",
    "class_names = [\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "    'del', 'nothing', 'space'\n",
    "]\n",
    "# !!! IMPORTANT: Use the size you trained with (128,128) or (200,200)\n",
    "IMG_SIZE = (128, 128) \n",
    "\n",
    "# --- 3. SET UP WEBCAM ---\n",
    "print(\"Opening camera...\")\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_V4L2) \n",
    "cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "print(\"Camera opened successfully.\")\n",
    "\n",
    "# --- 4. PREDICTION VARIABLES ---\n",
    "current_label = \"\"\n",
    "frame_counter = 0\n",
    "\n",
    "# --- 5. DEFINE SKIN COLOR RANGE (in HSV) ---\n",
    "# This is a common range for many skin tones.\n",
    "# You MAY need to adjust these values for your specific lighting.\n",
    "lower_skin = np.array([0, 48, 80], dtype=np.uint8)\n",
    "upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1) # Flip for selfie view\n",
    "    \n",
    "    # --- 6. FIND THE HAND (Skin-Tone Detection) ---\n",
    "    \n",
    "    # Convert frame to HSV (Hue, Saturation, Value) color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Create a \"mask\" that only keeps pixels within the skin-tone range\n",
    "    skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    \n",
    "    # \"Clean up\" the mask\n",
    "    # Erode \"shrinks\" the white areas, removing small noise\n",
    "    skin_mask = cv2.erode(skin_mask, np.ones((3,3), np.uint8), iterations=1)\n",
    "    # Dilate \"expands\" the white areas, closing gaps\n",
    "    skin_mask = cv2.dilate(skin_mask, np.ones((3,3), np.uint8), iterations=2)\n",
    "    \n",
    "    # Find the contours (outlines) of the \"blobs\" in the mask\n",
    "    contours, _ = cv2.findContours(skin_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # We assume the *largest* contour is the hand\n",
    "    if contours:\n",
    "        # Find the contour with the largest area\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Only proceed if the area is a reasonable size (not just noise)\n",
    "        if cv2.contourArea(c) > 1000:\n",
    "            # Get the bounding box (x, y, width, height) of the hand\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "            \n",
    "            # Draw the dynamic bounding box\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            \n",
    "            # --- 7. CROP AND PREDICT ---\n",
    "            frame_counter += 1\n",
    "            if frame_counter % 5 == 0:\n",
    "                try:\n",
    "                    # Crop the hand\n",
    "                    hand_roi = frame[y:y+h, x:x+w]\n",
    "                    \n",
    "                    # Pre-process this cropped image for *your* model\n",
    "                    img_for_model = cv2.resize(hand_roi, IMG_SIZE)\n",
    "                    img_for_model = cv2.cvtColor(img_for_model, cv2.COLOR_BGR2RGB) # BGR -> RGB\n",
    "                    img_rescaled = img_for_model / 255.0\n",
    "                    img_batch = np.expand_dims(img_rescaled, axis=0)\n",
    "                    \n",
    "                    # Make a prediction\n",
    "                    prediction = asl_model.predict(img_batch, verbose=0)\n",
    "                    pred_index = np.argmax(prediction[0])\n",
    "                    confidence = prediction[0][pred_index]\n",
    "                    \n",
    "                    if confidence > 0.5: # 50% confidence threshold\n",
    "                        pred_label = class_names[pred_index]\n",
    "                        current_label = f\"{pred_label} ({confidence*100:.2f}%)\"\n",
    "                    else:\n",
    "                        current_label = \"...\" \n",
    "                        \n",
    "                except Exception as e:\n",
    "                    current_label = \"...\"\n",
    "                    pass \n",
    "        else:\n",
    "            current_label = \"\" # Largest contour is too small\n",
    "    else:\n",
    "        current_label = \"\" # No contours found\n",
    "\n",
    "    # --- 8. DISPLAY THE PREDICTION ---\n",
    "    cv2.putText(\n",
    "        frame, current_label, (20, 40), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2\n",
    "    )\n",
    "    cv2.imshow('ASL Prediction (Skin-Tone Detection) - Press \"q\" to quit', frame)\n",
    "\n",
    "    # --- 9. SET UP EXIT KEY ---\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# --- 10. CLEAN UP ---\n",
    "print(\"Cleaning up and closing...\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa0a394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASL (conda:asl_gpu)",
   "language": "python",
   "name": "asl_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a666cb",
   "metadata": {},
   "source": [
    "# Cell 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7600201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 15:53:03.496179: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-11 15:53:04.699850: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-11 15:53:07.426248: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47238e3d",
   "metadata": {},
   "source": [
    "# Cell 2: Load the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973e345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading landmark dataset...\n",
      "Features shape: (69347, 63)\n",
      "Labels shape: (69347,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading landmark dataset...\")\n",
    "data = pd.read_csv('asl_landmarks_v2.csv')\n",
    "\n",
    "# Separate features (the 63 numbers) from labels\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\") # Should be (87000, 63)\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee82d0",
   "metadata": {},
   "source": [
    "# Cell 3: Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f465ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing labels...\n",
      "Data ready.\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing labels...\")\n",
    "# 1. Convert string labels ('A', 'B'...) to numbers (0, 1...)\n",
    "label_encoder = LabelEncoder()\n",
    "y_int = label_encoder.fit_transform(y)\n",
    "\n",
    "# 2. One-hot encode the numbers (e.g., 2 -> [0, 0, 1, 0...])\n",
    "num_classes = len(np.unique(y_int))\n",
    "y_categorical = utils.to_categorical(y_int, num_classes=num_classes)\n",
    "\n",
    "# 3. Create training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.values, # Convert DataFrame to numpy array\n",
    "    y_categorical, \n",
    "    test_size=0.2, # 20% for testing\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Data ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e55067",
   "metadata": {},
   "source": [
    "# Cell 4: Build the new, simple \"MLP\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3f0235d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762876422.605944  983363 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5561 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,741</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             │         \u001b[38;5;34m3,741\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">77,853</span> (304.11 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m77,853\u001b[0m (304.11 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">77,853</span> (304.11 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m77,853\u001b[0m (304.11 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    # Input layer expects a 1D vector of 63 numbers\n",
    "    layers.Input(shape=(63,)),\n",
    "    \n",
    "    # \"Brain\" of the model\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    \n",
    "    # Output layer: 29 classes, 'softmax' for probabilities\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88cdb0",
   "metadata": {},
   "source": [
    "# Cell 5: Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3d59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy', # Use 'categorical' (not 'sparse')\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3fc16",
   "metadata": {},
   "source": [
    "# Cell 6: Train the model (This will be VERY fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c26ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new landmark model...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 15:54:26.207903: I external/local_xla/xla/service/service.cc:163] XLA service 0x738dc001c9d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-11 15:54:26.207941: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-11-11 15:54:26.258845: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-11 15:54:26.504131: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91500\n",
      "2025-11-11 15:54:26.626972: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:26.627040: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:26.627177: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:26.627191: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:26.627204: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:26.627213: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:26.627221: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:27.730383: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_848', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-11-11 15:54:28.457337: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1075', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-11 15:54:29.683693: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_920', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-11 15:54:29.947158: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1126', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-11-11 15:54:30.067723: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1094', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-11 15:54:30.473050: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1094', 396 bytes spill stores, 396 bytes spill loads\n",
      "\n",
      "2025-11-11 15:54:30.596961: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1110', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-11-11 15:54:31.018973: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1094', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  57/1734\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.1046 - loss: 3.2449    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762876473.380590  983557 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1725/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6403 - loss: 1.1648"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 15:54:38.385288: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:38.385353: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:38.385381: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:38.385391: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:39.318621: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_848', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-11-11 15:54:39.406806: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_920', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-11 15:54:39.857071: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_848', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-11-11 15:54:40.781686: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1075', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6412 - loss: 1.1620"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 15:54:43.240815: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:43.240901: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 15:54:44.428935: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_82', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-11 15:54:44.454114: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_75', 100 bytes spill stores, 100 bytes spill loads\n",
      "\n",
      "2025-11-11 15:54:46.934823: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_82', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 8ms/step - accuracy: 0.7999 - loss: 0.6152 - val_accuracy: 0.9301 - val_loss: 0.2192\n",
      "Epoch 2/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9172 - loss: 0.2379 - val_accuracy: 0.9263 - val_loss: 0.1907\n",
      "Epoch 3/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9267 - loss: 0.2047 - val_accuracy: 0.9406 - val_loss: 0.1633\n",
      "Epoch 4/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9311 - loss: 0.1858 - val_accuracy: 0.9447 - val_loss: 0.1480\n",
      "Epoch 5/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9350 - loss: 0.1740 - val_accuracy: 0.9461 - val_loss: 0.1394\n",
      "Epoch 6/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9376 - loss: 0.1644 - val_accuracy: 0.9469 - val_loss: 0.1332\n",
      "Epoch 7/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9391 - loss: 0.1575 - val_accuracy: 0.9471 - val_loss: 0.1404\n",
      "Epoch 8/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9411 - loss: 0.1525 - val_accuracy: 0.9489 - val_loss: 0.1293\n",
      "Epoch 9/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9414 - loss: 0.1486 - val_accuracy: 0.9507 - val_loss: 0.1294\n",
      "Epoch 10/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9431 - loss: 0.1442 - val_accuracy: 0.9496 - val_loss: 0.1293\n",
      "Epoch 11/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9432 - loss: 0.1440 - val_accuracy: 0.9492 - val_loss: 0.1248\n",
      "Epoch 12/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9443 - loss: 0.1405 - val_accuracy: 0.9505 - val_loss: 0.1253\n",
      "Epoch 13/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9447 - loss: 0.1385 - val_accuracy: 0.9519 - val_loss: 0.1200\n",
      "Epoch 14/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9461 - loss: 0.1347 - val_accuracy: 0.9514 - val_loss: 0.1189\n",
      "Epoch 15/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9457 - loss: 0.1354 - val_accuracy: 0.9513 - val_loss: 0.1228\n",
      "Epoch 16/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9465 - loss: 0.1330 - val_accuracy: 0.9519 - val_loss: 0.1222\n",
      "Epoch 17/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9464 - loss: 0.1318 - val_accuracy: 0.9509 - val_loss: 0.1220\n",
      "Epoch 18/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9471 - loss: 0.1303 - val_accuracy: 0.9521 - val_loss: 0.1168\n",
      "Epoch 19/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9473 - loss: 0.1302 - val_accuracy: 0.9517 - val_loss: 0.1202\n",
      "Epoch 20/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9472 - loss: 0.1291 - val_accuracy: 0.9524 - val_loss: 0.1156\n",
      "Epoch 21/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9483 - loss: 0.1268 - val_accuracy: 0.9523 - val_loss: 0.1177\n",
      "Epoch 22/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9477 - loss: 0.1273 - val_accuracy: 0.9523 - val_loss: 0.1157\n",
      "Epoch 23/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9481 - loss: 0.1274 - val_accuracy: 0.9529 - val_loss: 0.1177\n",
      "Epoch 24/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9479 - loss: 0.1261 - val_accuracy: 0.9517 - val_loss: 0.1177\n",
      "Epoch 25/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9483 - loss: 0.1258 - val_accuracy: 0.9547 - val_loss: 0.1122\n",
      "Epoch 26/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9494 - loss: 0.1229 - val_accuracy: 0.9536 - val_loss: 0.1158\n",
      "Epoch 27/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9491 - loss: 0.1248 - val_accuracy: 0.9534 - val_loss: 0.1133\n",
      "Epoch 28/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9489 - loss: 0.1229 - val_accuracy: 0.9531 - val_loss: 0.1146\n",
      "Epoch 29/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9490 - loss: 0.1236 - val_accuracy: 0.9547 - val_loss: 0.1130\n",
      "Epoch 30/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9498 - loss: 0.1219 - val_accuracy: 0.9534 - val_loss: 0.1164\n",
      "Epoch 31/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9494 - loss: 0.1210 - val_accuracy: 0.9528 - val_loss: 0.1171\n",
      "Epoch 32/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9495 - loss: 0.1229 - val_accuracy: 0.9547 - val_loss: 0.1126\n",
      "Epoch 33/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9502 - loss: 0.1199 - val_accuracy: 0.9539 - val_loss: 0.1140\n",
      "Epoch 34/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9501 - loss: 0.1190 - val_accuracy: 0.9549 - val_loss: 0.1093\n",
      "Epoch 35/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9496 - loss: 0.1214 - val_accuracy: 0.9538 - val_loss: 0.1171\n",
      "Epoch 36/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9500 - loss: 0.1190 - val_accuracy: 0.9533 - val_loss: 0.1151\n",
      "Epoch 37/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9505 - loss: 0.1196 - val_accuracy: 0.9538 - val_loss: 0.1130\n",
      "Epoch 38/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9504 - loss: 0.1212 - val_accuracy: 0.9550 - val_loss: 0.1123\n",
      "Epoch 39/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9504 - loss: 0.1192 - val_accuracy: 0.9532 - val_loss: 0.1132\n",
      "Epoch 40/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9510 - loss: 0.1163 - val_accuracy: 0.9536 - val_loss: 0.1166\n",
      "Epoch 41/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9510 - loss: 0.1162 - val_accuracy: 0.9538 - val_loss: 0.1145\n",
      "Epoch 42/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9506 - loss: 0.1171 - val_accuracy: 0.9545 - val_loss: 0.1130\n",
      "Epoch 43/50\n",
      "\u001b[1m1734/1734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9509 - loss: 0.1171 - val_accuracy: 0.9544 - val_loss: 0.1115\n",
      "New landmark model trained and saved as 'asl_landmark_model.keras'!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training new landmark model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50, # We can use more epochs since it's so fast\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5),\n",
    "        tf.keras.callbacks.ModelCheckpoint('asl_landmark_model.keras', save_best_only=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"New landmark model trained and saved as 'asl_landmark_model.keras'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495046c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(asl_gpu)",
   "language": "python",
   "name": "asl_gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

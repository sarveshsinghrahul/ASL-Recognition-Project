{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c640ed58",
   "metadata": {},
   "source": [
    "## Code to check GPU integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107df115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:28:59.468840: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-11 12:29:00.340294: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-11 12:29:03.564782: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth for GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        print(\"Enabled memory growth for GPUs:\", gpus)\n",
    "    except Exception as e:\n",
    "        print(\"Could not set memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPUs found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4dcc04",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198d0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks, regularizers\n",
    "import datetime\n",
    "\n",
    "# --- NEW IMPORTS FOR TRANSFER LEARNING ---\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6993f1f",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7698039",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = str(\"asl_alphabet_train\")\n",
    "IMG_SIZE = (200, 200)\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "VAL_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc58e4",
   "metadata": {},
   "source": [
    "## Dataset imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9aee890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87000 files belonging to 29 classes.\n",
      "Using 73950 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762864152.819160  799113 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4950 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87000 files belonging to 29 classes.\n",
      "Using 13050 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# training dataset (subset=\"training\") and validation dataset (subset=\"validation\")\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d0131",
   "metadata": {},
   "source": [
    "## Class checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9880c136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found classes: 29\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(\"Found classes:\", len(class_names))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b26d1c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A                    3000\n",
      "B                    3000\n",
      "C                    3000\n",
      "D                    3000\n",
      "E                    3000\n",
      "F                    3000\n",
      "G                    3000\n",
      "H                    3000\n",
      "I                    3000\n",
      "J                    3000\n",
      "K                    3000\n",
      "L                    3000\n",
      "M                    3000\n",
      "N                    3000\n",
      "O                    3000\n",
      "P                    3000\n",
      "Q                    3000\n",
      "R                    3000\n",
      "S                    3000\n",
      "T                    3000\n",
      "U                    3000\n",
      "V                    3000\n",
      "W                    3000\n",
      "X                    3000\n",
      "Y                    3000\n",
      "Z                    3000\n",
      "del                  3000\n",
      "nothing              3000\n",
      "space                3000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "p = pathlib.Path(DATA_DIR)\n",
    "counts = {d.name: len(list(d.glob(\"*\"))) for d in p.iterdir() if d.is_dir()}\n",
    "for c, n in sorted(counts.items()):\n",
    "    print(f\"{c:20s} {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ac6951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.prefetch_op._PrefetchDataset"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a655cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of images_batch: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Images batch shape: (32, 200, 200, 3)\n",
      "Labels batch shape: (32,)\n",
      "\n",
      "--- Example from the batch ---\n",
      "Shape of one image: (200, 200, 3)\n",
      "Label for first image: 25\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(train_ds)\n",
    "\n",
    "first_batch = next(iterator)\n",
    "\n",
    "images_batch, labels_batch = first_batch\n",
    "\n",
    "\n",
    "print(f\"Type of images_batch: {type(images_batch)}\")\n",
    "print(f\"Images batch shape: {images_batch.shape}\")\n",
    "print(f\"Labels batch shape: {labels_batch.shape}\")\n",
    "\n",
    "print(\"\\n--- Example from the batch ---\")\n",
    "print(f\"Shape of one image: {images_batch[0].shape}\")\n",
    "print(f\"Label for first image: {labels_batch[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873c63b",
   "metadata": {},
   "source": [
    "## Optimize pipeline (for Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ec0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# We can use the same augmentation as before\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomRotation(0.1),  \n",
    "    tf.keras.layers.RandomZoom(0.1),    \n",
    "    tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2), \n",
    "    tf.keras.layers.RandomBrightness(0.1), \n",
    "    tf.keras.layers.RandomContrast(0.1)    \n",
    "])\n",
    "\n",
    "# --- Apply augmentation, THEN MobileNet's specific pre-processing ---\n",
    "# This function scales pixels from [0, 255] to [-1, 1]\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# --- Apply pre-processing to validation data ---\n",
    "val_ds = val_ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "312022c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_799113/1955976156.py:6: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,877</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             │        \u001b[38;5;34m14,877\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,733</span> (11.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,928,733\u001b[0m (11.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">670,749</span> (2.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m670,749\u001b[0m (2.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_classes = len(class_names)\n",
    "input_shape = (*IMG_SIZE, 3) \n",
    "\n",
    "# 1. Load the \"Street Smart\" Base Model (MobileNetV2)\n",
    "# We don't include its final \"top\" layer\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=input_shape,\n",
    "    include_top=False, \n",
    "    weights='imagenet' # Load the knowledge from millions of images\n",
    ")\n",
    "\n",
    "# 2. FREEZE the expert's knowledge. \n",
    "# We don't want to re-train the part that knows about edges and textures.\n",
    "base_model.trainable = False\n",
    "\n",
    "# 3. Build your new model on top\n",
    "model = models.Sequential([\n",
    "    # Start with the frozen expert\n",
    "    base_model,\n",
    "    \n",
    "    # Add our own classifier head\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3), # A dropout of 0.3 is a good starting point\n",
    "    layers.Dense(num_classes, activation='softmax') # Your 29-class output\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "875f183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy', # <-- Perfect for integer labels!\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e297df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a log directory for this specific run\n",
    "# The datetime string makes each run unique in TensorBoard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Create the TensorBoard callback\n",
    "tensorboard_callback = callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1  # This logs weight histograms every epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04994072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback to stop training if validation accuracy doesn't improve\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3, # Stop after 3 epochs of no improvement\n",
    "    restore_best_weights=True # Automatically restore the best model weights\n",
    ")\n",
    "\n",
    "# Create a callback to save your best model to a file\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    'model1.keras', # File name\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e922d7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:29:31.335992: I external/local_xla/xla/service/service.cc:163] XLA service 0x76f4c4002c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-11 12:29:31.336037: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-11-11 12:29:31.693912: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-11 12:29:33.138685: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91500\n",
      "2025-11-11 12:29:33.340270: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 12:29:33.340353: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 12:29:34.264509: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6681', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-11 12:29:35.158339: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6681', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-11-11 12:29:35.184238: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4437', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-11 12:29:35.566324: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4437', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2025-11-11 12:29:43.162518: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-11-11 12:29:43.365955: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "I0000 00:00:1762864185.837755  799243 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2309/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4702 - loss: 1.8103"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:31:01.423675: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 12:31:03.022824: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4437', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-11 12:31:03.169636: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4437', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2025-11-11 12:31:10.220061: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng3{k11=0} for conv (f32[30,576,13,13]{3,2,1,0}, u8[0]{0}) custom-call(f32[30,576,13,13]{3,2,1,0}, f32[576,1,3,3]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=576, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
      "2025-11-11 12:31:10.311128: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.07922051s\n",
      "Trying algorithm eng3{k11=0} for conv (f32[30,576,13,13]{3,2,1,0}, u8[0]{0}) custom-call(f32[30,576,13,13]{3,2,1,0}, f32[576,1,3,3]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=576, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
      "2025-11-11 12:31:11.301554: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-11-11 12:31:11.502601: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4704 - loss: 1.8099"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:31:17.642590: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 12:31:19.043117: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1176', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-11 12:31:19.106245: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1176', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-11 12:31:19.614400: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1176', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-11 12:31:19.624142: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1176', 332 bytes spill stores, 332 bytes spill loads\n",
      "\n",
      "2025-11-11 12:31:30.741635: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-11 12:31:31.893631: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1176', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-11 12:31:32.343005: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1176', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-11 12:31:32.886585: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1176', 332 bytes spill stores, 332 bytes spill loads\n",
      "\n",
      "2025-11-11 12:31:39.456838: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-11-11 12:31:39.666506: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 51ms/step - accuracy: 0.5968 - loss: 1.3167 - val_accuracy: 0.8283 - val_loss: 0.4947\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:31:45.068316: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2309/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7091 - loss: 0.9118"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:32:59.272564: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 36ms/step - accuracy: 0.7206 - loss: 0.8727 - val_accuracy: 0.8598 - val_loss: 0.4070\n",
      "Epoch 3/20\n",
      "\u001b[1m   3/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:33\u001b[0m 67ms/step - accuracy: 0.7066 - loss: 0.8484 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:33:09.378885: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 36ms/step - accuracy: 0.7483 - loss: 0.7697 - val_accuracy: 0.8716 - val_loss: 0.3736\n",
      "Epoch 4/20\n",
      "\u001b[1m   2/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:04\u001b[0m 80ms/step - accuracy: 0.7812 - loss: 0.7068  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:34:33.132309: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 34ms/step - accuracy: 0.7637 - loss: 0.7194 - val_accuracy: 0.8888 - val_loss: 0.3166\n",
      "Epoch 5/20\n",
      "\u001b[1m   2/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:14\u001b[0m 84ms/step - accuracy: 0.7656 - loss: 0.7446  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:35:52.546766: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 35ms/step - accuracy: 0.7782 - loss: 0.6792 - val_accuracy: 0.8907 - val_loss: 0.3182\n",
      "Epoch 6/20\n",
      "\u001b[1m   2/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:23\u001b[0m 88ms/step - accuracy: 0.7031 - loss: 0.8517  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:37:14.777273: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 34ms/step - accuracy: 0.7835 - loss: 0.6608 - val_accuracy: 0.8766 - val_loss: 0.3504\n",
      "Epoch 7/20\n",
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 36ms/step - accuracy: 0.7962 - loss: 0.6270 - val_accuracy: 0.8998 - val_loss: 0.2957\n",
      "Epoch 8/20\n",
      "\u001b[1m   3/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:25\u001b[0m 63ms/step - accuracy: 0.7517 - loss: 0.8717 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:39:57.047879: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 34ms/step - accuracy: 0.7935 - loss: 0.6243 - val_accuracy: 0.8996 - val_loss: 0.2971\n",
      "Epoch 9/20\n",
      "\u001b[1m   2/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:40\u001b[0m 95ms/step - accuracy: 0.7969 - loss: 0.5122  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:41:15.574792: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 36ms/step - accuracy: 0.8005 - loss: 0.6020 - val_accuracy: 0.9068 - val_loss: 0.2650\n",
      "Epoch 10/20\n",
      "\u001b[1m   3/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 57ms/step - accuracy: 0.8333 - loss: 0.5927 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:42:40.022129: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 37ms/step - accuracy: 0.8082 - loss: 0.5871 - val_accuracy: 0.9108 - val_loss: 0.2550\n",
      "Epoch 11/20\n",
      "\u001b[1m   2/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:30\u001b[0m 65ms/step - accuracy: 0.7969 - loss: 0.4694  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:44:05.328055: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 37ms/step - accuracy: 0.8102 - loss: 0.5767 - val_accuracy: 0.9047 - val_loss: 0.2765\n",
      "Epoch 12/20\n",
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 37ms/step - accuracy: 0.8119 - loss: 0.5653 - val_accuracy: 0.9143 - val_loss: 0.2479\n",
      "Epoch 13/20\n",
      "\u001b[1m   1/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26:18\u001b[0m 683ms/step - accuracy: 0.8750 - loss: 0.2650"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:46:57.713446: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 37ms/step - accuracy: 0.8152 - loss: 0.5629 - val_accuracy: 0.9145 - val_loss: 0.2516\n",
      "Epoch 14/20\n",
      "\u001b[1m   2/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:52\u001b[0m 101ms/step - accuracy: 0.8516 - loss: 0.4621 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:48:23.366783: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 37ms/step - accuracy: 0.8181 - loss: 0.5510 - val_accuracy: 0.9200 - val_loss: 0.2338\n",
      "Epoch 15/20\n",
      "\u001b[1m   2/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:09\u001b[0m 82ms/step - accuracy: 0.8750 - loss: 0.5578  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:49:49.902726: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 36ms/step - accuracy: 0.8212 - loss: 0.5414 - val_accuracy: 0.9067 - val_loss: 0.2831\n",
      "Epoch 16/20\n",
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 37ms/step - accuracy: 0.8241 - loss: 0.5371 - val_accuracy: 0.9103 - val_loss: 0.2618\n",
      "Epoch 17/20\n",
      "\u001b[1m   2/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 61ms/step - accuracy: 0.8281 - loss: 0.7109  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:52:38.815211: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2310/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8252 - loss: 0.5354"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:53:53.449993: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 36ms/step - accuracy: 0.8269 - loss: 0.5276 - val_accuracy: 0.9266 - val_loss: 0.2084\n",
      "Epoch 18/20\n",
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 36ms/step - accuracy: 0.8281 - loss: 0.5233 - val_accuracy: 0.8998 - val_loss: 0.2985\n",
      "Epoch 19/20\n",
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 38ms/step - accuracy: 0.8301 - loss: 0.5163 - val_accuracy: 0.9267 - val_loss: 0.2157\n",
      "Epoch 20/20\n",
      "\u001b[1m   2/2311\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:17\u001b[0m 85ms/step - accuracy: 0.8125 - loss: 0.4995  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:56:55.641874: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 15360256 bytes after encountering the first element of size 15360256 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 36ms/step - accuracy: 0.8308 - loss: 0.5150 - val_accuracy: 0.9288 - val_loss: 0.2051\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20 # Start with 20, EarlyStopping will stop it if it's done sooner\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[early_stopping,\n",
    "     model_checkpoint,\n",
    "     tensorboard_callback\n",
    "     ] # Pass in our helpers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb704a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASL (conda:asl_gpu)",
   "language": "python",
   "name": "asl_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

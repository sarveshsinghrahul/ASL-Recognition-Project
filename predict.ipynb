{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9058da2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 07:23:17.529471: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-12 07:23:17.827963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762932197.936107   24600 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762932197.967948   24600 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762932198.201775   24600 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762932198.201835   24600 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762932198.201836   24600 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762932198.201838   24600 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-12 07:23:18.229713: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading ASL model: File not found: filepath=models\u0007sl_landmark_model.keras. Please ensure the file is an accessible `.keras` zip file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762932202.558843   24600 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1762932202.614742   24713 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: llvmpipe (LLVM 20.1.2, 256 bits)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1762932202.671029   24666 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1762932202.702715   24673 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MediaPipe (Old API) Hand Detector created.\n",
      "Opening camera...\n",
      "Camera opened successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1762932203.467873   24668 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'asl_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 83\u001b[0m\n\u001b[1;32m     79\u001b[0m     mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(\n\u001b[1;32m     80\u001b[0m         frame, hand_landmarks, mp_hands\u001b[38;5;241m.\u001b[39mHAND_CONNECTIONS)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# --- 8. MAKE A PREDICTION (ALWAYS) ---\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43masl_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(model_input, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     84\u001b[0m pred_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prediction[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# --- 9. SMOOTH THE PREDICTION ---\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asl_model' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# predict_live_cpu.py\n",
    "# RUN THIS IN YOUR \"asl_cpu_env\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp # Uses the OLD solutions API\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# --- 1. DEFINE CONSTANTS ---\n",
    "ASL_MODEL_PATH = 'models/asl_landmark_model.keras' # Your landmark model\n",
    "class_names = [\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "    'del', 'nothing', 'space'\n",
    "]\n",
    "ZERO_VECTOR_NP = np.array([0.0] * 63, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "# --- 2. LOAD YOUR CLASSIFIER MODEL ---\n",
    "try:\n",
    "    asl_model = load_model(ASL_MODEL_PATH)\n",
    "    print(\"ASL Landmark model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ASL model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. INITIALIZE MEDIAPIPE (OLD API) ---\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False, \n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7, \n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "print(\"MediaPipe (Old API) Hand Detector created.\")\n",
    "\n",
    "# --- 4. SET UP WEBCAM ---\n",
    "print(\"Opening camera...\")\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_V4L2) \n",
    "cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P','G'))\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "print(\"Camera opened successfully.\")\n",
    "\n",
    "# --- 5. PREDICTION & SMOOTHING VARIABLES ---\n",
    "predictions_deque = deque(maxlen=10) # Holds the last 10 predictions\n",
    "current_label = \"\"\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # --- 6. MEDIAPIPE HAND DETECTION ---\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "    \n",
    "    model_input = ZERO_VECTOR_NP\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        \n",
    "        # --- 7. NORMALIZE LANDMARKS (Same as in training) ---\n",
    "        wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "        landmark_vector = []\n",
    "        for landmark in hand_landmarks.landmark:\n",
    "            landmark_vector.append(landmark.x - wrist.x)\n",
    "            landmark_vector.append(landmark.y - wrist.y)\n",
    "            landmark_vector.append(landmark.z - wrist.z)\n",
    "        \n",
    "        model_input = np.array(landmark_vector, dtype=np.float32).reshape(1, -1)\n",
    "        \n",
    "        # Draw skeleton on the frame\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "    # --- 8. MAKE A PREDICTION (ALWAYS) ---\n",
    "    prediction = asl_model.predict(model_input, verbose=0)\n",
    "    pred_index = np.argmax(prediction[0])\n",
    "    \n",
    "    # --- 9. SMOOTH THE PREDICTION ---\n",
    "    predictions_deque.append(pred_index)\n",
    "    \n",
    "    if len(predictions_deque) == 10:\n",
    "        most_common_pred = np.bincount(predictions_deque).argmax()\n",
    "        confidence = prediction[0][most_common_pred]\n",
    "        \n",
    "        if confidence > 0.6: # 60% confidence\n",
    "            current_label = class_names[most_common_pred]\n",
    "        else:\n",
    "            if class_names[most_common_pred] == 'nothing':\n",
    "                 current_label = \"nothing\"\n",
    "            else:\n",
    "                 current_label = \"...\"\n",
    "    else:\n",
    "        current_label = \"...\"\n",
    "\n",
    "    cv2.putText(\n",
    "        frame, current_label, (20, 40), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2\n",
    "    )\n",
    "    cv2.imshow('ASL CPU Prediction - Press \"q\" to quit', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# --- 10. CLEAN UP ---\n",
    "print(\"Cleaning up and closing...\")\n",
    "hands.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db979e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "âœ… CPU-Only ASL (asl_cpu)",
   "language": "python",
   "name": "asl_cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

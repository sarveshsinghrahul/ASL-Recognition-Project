{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74df9b97",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b98bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1eaccb",
   "metadata": {},
   "source": [
    "### --- 1. INITIALIZE MEDIAPIPE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762870932.738629  966166 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1762870932.814064  966434 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: llvmpipe (LLVM 20.1.2, 256 bits)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True, # We are processing static images\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93257ece",
   "metadata": {},
   "source": [
    "### --- 2. DEFINE CLASSES AND DATA FOLDER ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb1c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'asl_alphabet_train'\n",
    "class_names = [\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "    'del', 'nothing', 'space'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa1b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening asl_landmarks.csv to write...\n",
      "Processing 3000 images for class: A\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "_graph is None in SolutionBase",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m rgb_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Process the image to find hands\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# --- 6. EXTRACT & NORMALIZE LANDMARKS ---\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n",
      "File \u001b[0;32m~/miniforge3/envs/env_mediapipe/lib/python3.10/site-packages/mediapipe/python/solutions/hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/env_mediapipe/lib/python3.10/site-packages/mediapipe/python/solution_base.py:316\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m33333\u001b[39m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_graph is None in SolutionBase\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stream_name, data \u001b[38;5;129;01min\u001b[39;00m input_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    318\u001b[0m   input_stream_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_stream_type_info[stream_name]\n",
      "\u001b[0;31mValueError\u001b[0m: _graph is None in SolutionBase"
     ]
    }
   ],
   "source": [
    "# --- 3. CREATE THE NEW CSV FILE ---\n",
    "csv_file_name = 'asl_landmarks.csv'\n",
    "print(f\"Opening {csv_file_name} to write...\")\n",
    "\n",
    "with open(csv_file_name, mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    # --- 4. WRITE THE HEADER ROW ---\n",
    "    # This is the 'label' column, plus 63 coordinate columns (x0, y0, z0, x1, y1, z1, ...)\n",
    "    header = ['label']\n",
    "    for i in range(21): # 21 landmarks\n",
    "        header.extend([f'x{i}', f'y{i}', f'z{i}'])\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # --- 5. LOOP THROUGH ALL IMAGES ---\n",
    "    for label in class_names:\n",
    "        folder_path = os.path.join(DATA_DIR, label, '*.jpg')\n",
    "        image_files = glob.glob(folder_path)\n",
    "        print(f\"Processing {len(image_files)} images for class: {label}\")\n",
    "        \n",
    "        for image_path in image_files:\n",
    "            # Load the image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "                \n",
    "            # Convert BGR (OpenCV) to RGB (MediaPipe)\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process the image to find hands\n",
    "            results = hands.process(rgb_image)\n",
    "\n",
    "            # --- 6. EXTRACT & NORMALIZE LANDMARKS ---\n",
    "            if results.multi_hand_landmarks:\n",
    "                hand_landmarks = results.multi_hand_landmarks[0] # Get the first (and only) hand\n",
    "                \n",
    "                # --- THIS IS THE \"SECRET SAUCE\" ---\n",
    "                # We normalize all landmarks to be relative to the wrist (landmark 0).\n",
    "                # This makes the data \"translation-invariant\" (it doesn't matter\n",
    "                # where the hand is on the screen).\n",
    "                \n",
    "                wrist = hand_landmarks.landmark[0]\n",
    "                landmark_vector = []\n",
    "                \n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    # Calculate coordinates relative to the wrist\n",
    "                    landmark_vector.append(landmark.x - wrist.x)\n",
    "                    landmark_vector.append(landmark.y - wrist.y)\n",
    "                    landmark_vector.append(landmark.z - wrist.z)\n",
    "                    \n",
    "                # landmark_vector is now a list of 63 numbers\n",
    "                \n",
    "                # --- 7. WRITE TO CSV ---\n",
    "                # The first column is the label, followed by the 63 numbers\n",
    "                csv_writer.writerow([label] + landmark_vector)\n",
    "\n",
    "print(\"--- Dataset Creation Complete! ---\")\n",
    "hands.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b00f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "âœ… MediaPipe Sender (env_mediapipe)",
   "language": "python",
   "name": "env_mediapipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
